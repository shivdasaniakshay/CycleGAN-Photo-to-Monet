{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79545aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n",
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from IPython.display import clear_output\n",
    "import pathlib\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "print(tf.__version__)\n",
    "\n",
    "os.chdir('/Users/akshay/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c365de42-d915-492f-ba23-bdcffd6ba1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Config\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TRAIN_DIR = 'data/train'\n",
    "VAL_DIR = 'data/val'\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-4\n",
    "LAMBDA_IDENTITY = 0\n",
    "LAMBDA_CYCLE = 10 \n",
    "NUM_WORKERS = 0\n",
    "NUM_EPOCHS = 6\n",
    "LOAD_MODEL = False\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_H = 'genh.pth.tar'\n",
    "CHECKPOINT_GEN_Z = 'genz.pth.tar'\n",
    "CHECKPOINT_CRITIC_H = 'critich.pth.tar'\n",
    "CHECKPOINT_CRITIC_Z = 'criticz.pth.tar'\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(width=256, height=256),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(p=0.1),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255),\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    additional_targets={'image0': 'image'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d5b3304-53ea-450e-8990-fa1edd5e1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhotoMonetDataset(Dataset):\n",
    "    def __init__(self, root_monet, root_photo, transform=None):\n",
    "        self.root_monet = root_monet\n",
    "        self.root_photo = root_photo\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.monet_images = os.listdir(root_monet)\n",
    "        self.photo_images = os.listdir(root_photo)\n",
    "        self.length_dataset = max(len(self.photo_images), len(self.monet_images))\n",
    "        self.monet_len = len(self.monet_images)\n",
    "        self.photo_len = len(self.photo_images)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length_dataset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        monet_img = self.monet_images[index % self.monet_len]\n",
    "        photo_img = self.photo_images[index % self.photo_len]\n",
    "        \n",
    "        monet_path = os.path.join(self.root_monet, monet_img)\n",
    "        photo_path = os.path.join(self.root_photo, photo_img)\n",
    "        \n",
    "        monet_img = np.array(Image.open(monet_path).convert('RGB'))\n",
    "        photo_img = np.array(Image.open(photo_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=monet_img, image0=photo_img)\n",
    "            monet_img = augmentations['image']\n",
    "            photo_img = augmentations['image0']\n",
    "            \n",
    "        return monet_img, photo_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17edfef7-7d95-4f26-a867-0797b8b37b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9567a2c3-ce16-4fa4-af81-1122bd48bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator\n",
    "\n",
    "#Blocks are fundamental building block of ML models\n",
    "#Typically consisting of 1+ neurons that work to process input data and produce output\n",
    "#Tensor: A data structure. 1D tensor is vector, 2D tensor is matrix.\n",
    "\n",
    "class Block(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels, stride): #constuctor method, in_channels are number of channels in input image, out_channels denote number of channels produced by convolution\n",
    "        super().__init__() #Calls constructor of parent class\n",
    "        self.conv = nn.Sequential( #defines variable self.conv, which is sequential container that provide a way to build sequence of layes in neural network to organize flow of data through layers\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias = True, padding_mode = \"reflect\"), #Creates 2D convolutional layer, kernel size of 4, padding of 1, padding mode reflective\n",
    "            nn.InstanceNorm2d(out_channels), #adds instance normalization layer, which helps to normalize activations within each channel, improving training stability and convergence\n",
    "            nn.LeakyReLU(0.2, inplace=True) #Adds leaky ReLU activation function with negative slope of 0.2, allowing small negative values to pass through, introducing some non-linearity while preventing vanishing gradient problem\n",
    "        )\n",
    "    \n",
    "    #Defines how data flows through the layers of module during forward pass (process of moving data through model from input -> output)\n",
    "    def forward(self, x): \n",
    "        return self.conv(x)\n",
    "    \n",
    "#Discriminator - Task is to classify whether a given input image is real or fake\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features = [64, 128, 256, 512]): #Constructor method, in_channels=3 for RBG, features are number of output channels in each layer of discriminator\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential( #Defines initial part of discirminator network, which consists of 2 layers\n",
    "            nn.Conv2d( #2D convolutional layer, features[0] output channels\n",
    "                in_channels,\n",
    "                features[0],\n",
    "                kernel_size = 4,\n",
    "                stride = 2,\n",
    "                padding = 1,\n",
    "                padding_mode = \"reflect\"\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True) #Leaky ReLU actiovation function\n",
    "        )\n",
    "        \n",
    "        layers = [] #Used to hold subsequent layers of discriminator\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]: #Iterates over remaining elements in features list, excluding first one as it was used in initial layer\n",
    "            layers.append(Block(in_channels, feature, stride = 1 if feature==features[-1] else 2)) #Appends Block to layers list, used to create sequence of convolutional layers with increasing features\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size = 4, stride = 1, padding = 1, padding_mode = 'reflect')) #Adds final 2D convolutional layer that takes output from previous layers, used for final classification\n",
    "        self.model = nn.Sequential(*layers) #Creates final discriminator model, * unpacks list, pasing each element of list as seperate arguments to nn.Sequential\n",
    "\n",
    "    #Applies inital layers to input fata followed by main layers of discriminator, output passed through torch.sigmoid to produce final output\n",
    "    #Representing discriminator's confidence in classifying input data as real or fake\n",
    "    def forward(self, x): \n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80193a87-57d6-497f-844e-089432508e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                img_channels,\n",
    "                num_features,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "                padding_mode=\"reflect\",\n",
    "            ),\n",
    "            nn.InstanceNorm2d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    num_features, num_features * 2, kernel_size=3, stride=2, padding=1\n",
    "                ),\n",
    "                ConvBlock(\n",
    "                    num_features * 2,\n",
    "                    num_features * 4,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features * 4) for _ in range(num_residuals)]\n",
    "        )\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(\n",
    "                    num_features * 4,\n",
    "                    num_features * 2,\n",
    "                    down=False,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "                ConvBlock(\n",
    "                    num_features * 2,\n",
    "                    num_features * 1,\n",
    "                    down=False,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    output_padding=1,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last = nn.Conv2d(\n",
    "            num_features * 1,\n",
    "            img_channels,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=3,\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.res_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aec2cdf3-ad8d-4d82-9620-e95b7aa31e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████| 1407/1407 [4:50:02<00:00, 12.37s/it, H_fake=0.329, H_real=0.828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1407/1407 [2:23:22<00:00,  6.11s/it, H_fake=0.16, H_real=0.933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████| 1407/1407 [2:22:13<00:00,  6.06s/it, H_fake=0.162, H_real=0.918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 1407/1407 [2:09:28<00:00,  5.52s/it, H_fake=0.0703, H_real=0.944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 1407/1407 [2:09:35<00:00,  5.53s/it, H_fake=0.0426, H_real=0.965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████| 1407/1407 [2:09:38<00:00,  5.53s/it, H_fake=0.0227, H_real=0.983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n",
      "=> Saving checkpoint\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "def train_fn(\n",
    "    disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler\n",
    "):\n",
    "    H_reals = 0\n",
    "    H_fakes = 0\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (monet, photo) in enumerate(loop):\n",
    "        monet = monet.to(DEVICE)\n",
    "        photo = photo.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake_photo = gen_H(monet)\n",
    "            D_H_real = disc_H(photo)\n",
    "            D_H_fake = disc_H(fake_photo.detach())\n",
    "            H_reals += D_H_real.mean().item()\n",
    "            H_fakes += D_H_fake.mean().item()\n",
    "            D_H_real_loss = mse(D_H_real, torch.ones_like(D_H_real))\n",
    "            D_H_fake_loss = mse(D_H_fake, torch.zeros_like(D_H_fake))\n",
    "            D_H_loss = D_H_real_loss + D_H_fake_loss\n",
    "\n",
    "            fake_monet = gen_Z(monet)\n",
    "            D_Z_real = disc_Z(monet)\n",
    "            D_Z_fake = disc_Z(fake_monet.detach())\n",
    "            D_Z_real_loss = mse(D_Z_real, torch.ones_like(D_Z_real))\n",
    "            D_Z_fake_loss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))\n",
    "            D_Z_loss = D_Z_real_loss + D_Z_fake_loss\n",
    "\n",
    "            # put it togethor\n",
    "            D_loss = (D_H_loss + D_Z_loss) / 2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generators H and Z\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # adversarial loss for both generators\n",
    "            D_H_fake = disc_H(fake_photo)\n",
    "            D_Z_fake = disc_Z(fake_monet)\n",
    "            loss_G_H = mse(D_H_fake, torch.ones_like(D_H_fake))\n",
    "            loss_G_Z = mse(D_Z_fake, torch.ones_like(D_Z_fake))\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_monet = gen_Z(fake_photo)\n",
    "            cycle_photo = gen_H(fake_monet)\n",
    "            cycle_monet_loss = l1(monet, cycle_monet)\n",
    "            cycle_photo_loss = l1(photo, cycle_photo)\n",
    "\n",
    "            # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
    "            identity_monet = gen_Z(monet)\n",
    "            identity_photo = gen_H(photo)\n",
    "            identity_monet_loss = l1(monet, identity_monet)\n",
    "            identity_photo_loss = l1(photo, identity_monet)\n",
    "\n",
    "            # add all togethor\n",
    "            G_loss = (\n",
    "                loss_G_Z\n",
    "                + loss_G_H\n",
    "                + cycle_monet_loss * LAMBDA_CYCLE\n",
    "                + cycle_photo_loss * LAMBDA_CYCLE\n",
    "                + identity_photo_loss * LAMBDA_IDENTITY\n",
    "                + identity_monet_loss * LAMBDA_IDENTITY\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            save_image(fake_photo * 0.5 + 0.5, f\"saved_images/photo_{idx}.png\")\n",
    "            save_image(fake_monet * 0.5 + 0.5, f\"saved_images/monet_{idx}.png\")\n",
    "\n",
    "        loop.set_postfix(H_real=H_reals / (idx + 1), H_fake=H_fakes / (idx + 1))\n",
    "\n",
    "\n",
    "def main():\n",
    "    disc_H = Discriminator(in_channels=3).to(DEVICE)\n",
    "    disc_Z = Discriminator(in_channels=3).to(DEVICE)\n",
    "    gen_Z = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "    gen_H = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "    opt_disc = optim.Adam(\n",
    "        list(disc_H.parameters()) + list(disc_Z.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    opt_gen = optim.Adam(\n",
    "        list(gen_Z.parameters()) + list(gen_H.parameters()),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.5, 0.999),\n",
    "    )\n",
    "\n",
    "    L1 = nn.L1Loss()\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_H,\n",
    "            gen_H,\n",
    "            opt_gen,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_GEN_Z,\n",
    "            gen_Z,\n",
    "            opt_gen,\n",
    "            config.LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_H,\n",
    "            disc_H,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_CRITIC_Z,\n",
    "            disc_Z,\n",
    "            opt_disc,\n",
    "            LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "    dataset = PhotoMonetDataset(\n",
    "        root_photo=TRAIN_DIR + \"/photos\",\n",
    "        root_monet=TRAIN_DIR + \"/monets\",\n",
    "        transform=transforms,\n",
    "    )\n",
    "    val_dataset = PhotoMonetDataset(\n",
    "        root_photo=VAL_DIR + \"/photos\",\n",
    "        root_monet=VAL_DIR + \"/monets\",\n",
    "        transform=transforms,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    g_scaler = torch.cuda.amp.GradScaler()\n",
    "    d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_fn(\n",
    "            disc_H,\n",
    "            disc_Z,\n",
    "            gen_Z,\n",
    "            gen_H,\n",
    "            loader,\n",
    "            opt_disc,\n",
    "            opt_gen,\n",
    "            L1,\n",
    "            mse,\n",
    "            d_scaler,\n",
    "            g_scaler\n",
    "        )\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_checkpoint(gen_H, opt_gen, filename=CHECKPOINT_GEN_H)\n",
    "            save_checkpoint(gen_Z, opt_gen, filename=CHECKPOINT_GEN_Z)\n",
    "            save_checkpoint(disc_H, opt_disc, filename=CHECKPOINT_CRITIC_H)\n",
    "            save_checkpoint(disc_Z, opt_disc, filename=CHECKPOINT_CRITIC_Z)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "088b4cf7-3032-409a-8ff9-3c732cc65173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_generated_samples(ds, model, n_samples):\n",
    "    ds_iter = iter(ds)\n",
    "    for n_sample in range(n_samples):\n",
    "        example_sample = next(ds_iter)\n",
    "        generated_sample = model.predict(example_sample)\n",
    "    \n",
    "        plt.subplot(121)\n",
    "        plt.title(\"Input image\")\n",
    "        plt.imshow(example_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.title(\"Generated image\")\n",
    "        plt.imshow(generated_sample[0] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "MONET_FILENAMES = tf.io.gfile.glob(str('/Users/akshay/Downloads/gan-getting-started/monet_jpg/*.jpg'))\n",
    "PHOTO_FILENAMES = tf.io.gfile.glob(str('/Users/akshay/Downloads/gan-getting-started/photo_jpg/*.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
